import pdb
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, RobertaTokenizer, GPT2Tokenizer
from transformers import Trainer, Seq2SeqTrainer

class DebugTrainer(Trainer):
    def __init__(self, *args, **kwargs):

        super().__init__(*args, **kwargs)

    def compute_loss(self, model, inputs, return_outputs=False):
        loss = super().compute_loss(model, inputs, return_outputs=return_outputs)
        # try:
        #     if torch.isnan(loss):
        #         pdb.set_trace()
        # except:
        #     if torch.isnan(loss[0]):
        #         pdb.set_trace()
        return loss
if __name__ == "__main__":
    # tokenizer = GPT2Tokenizer.from_pretrained("/brtx/601-nvme1/estengel/.cache/codet5-base")
    model = AutoModelForSeq2SeqLM.from_pretrained("/brtx/601-nvme1/estengel/.cache/codet5-base")

    # input = "this is an input: 1 + 1"*256
    # output = "this is an output: 2 + 2"*256

    # input_ids = tokenizer(input, return_tensors="pt").input_ids
    # output_ids = tokenizer(output, return_tensors="pt").input_ids

    # input_ids = torch.tensor([262, 269, 314, 269, 1223, 269, 436, 269, 366, 269, 1645, 269, 14543, 269, 15407, 269, 7138, 77, 269, 2393, 269, 2873, 269, 7129, 269, 1427, 269, 9834, 70, 269, 366, 17152, 269, 699, 269, 9033, 269, 314, 67, 350, 84, 571, 7291, 67, 1293, 298, 908, 294, 7291, 67, 350, 269, 645, 298, 908, 67, 350, 269, 3286, 571, 652, 310, 294, 7291, 67, 350, 269, 3286, 269, 384, 474, 269, 5927, 67, 350, 261, 432, 5967, 262, 269, 884, 20910, 67, 350, 261, 18520, 262, 269, 949, 269, 314, 269, 10763, 269, 316, 82, 67, 20125, 269, 8275, 269, 279, 269, 425, 269, 9986, 269, 6386, 269, 6357, 269, 2393, 269, 2873, 269, 998, 86, 571, 652, 310, 67, 659, 1518, 294, 7291, 67, 350, 269, 3286, 269, 384, 474, 269, 5118, 74, 269, 314, 8522, 269, 3821, 74, 571, 652, 310, 67, 2767, 30321, 294, 7291, 67, 350, 269, 3286, 269, 5927, 67, 350, 261, 432, 5967, 262, 269, 884, 20910, 67, 350, 261, 18520, 262, 269, 3643, 269, 949, 269, 314, 269, 10763, 269, 316, 82, 67, 20125, 269, 8275, 269, 279, 269, 425, 269, 9986, 269, 8071, 269, 6386, 269, 2393, 269, 2873, 571, 366, 454, 67, 792, 67, 74, 339, 294, 7291, 67, 350, 269, 3286, 350, 269, 331, 16474, 1637, 269, 26503, 6968, 269, 3577, 269, 19588, 269, 1547, 853, 329, 269, 3150, 261, 19185, 262, 269, 3577, 67, 7652, 571, 6382, 67, 13957, 294, 3286, 269, 884, 20910, 67, 350, 261, 18520, 262, 269, 5927, 67, 350, 261, 432, 5967, 262, 269, 779, 79, 67, 350, 269, 4548, 67, 3645, 269, 4548, 67, 2722, 269, 28422, 269, 1696, 899, 269, 2403, 409, 1359, 571, 3301, 294, 7291, 67, 350, 269, 3286, 269, 5927, 67, 350, 261, 432, 5967, 262, 269, 884, 20910, 67, 350, 261, 18520, 262, 269, 316, 30321, 269, 314, 269, 341, 269, 328, 269, 6171, 269, 293, 715, 86, 67, 13552, 571, 3301, 67, 20222, 294, 7291, 67, 350, 269, 3286, 269, 5927, 67, 350, 261, 432, 5967, 262, 269, 884, 20910, 67, 350, 261, 18520, 262, 269, 316, 30321, 269, 8816, 269, 314, 269, 341, 269, 328, 269, 6171, 571, 7291, 294, 7291, 67, 350, 269, 17057, 67, 6874, 269, 17057, 67, 7496, 269, 17057, 67, 2881, 269, 17057, 67, 9082, 269, 17057, 67, 2019, 269, 17057, 67, 14240, 269, 443, 421, 67, 6874, 269, 443, 421, 67, 7496, 269, 443, 421, 67, 2881, 269, 443, 421, 67, 9082, 269, 443, 421, 67, 2019, 269, 443, 421, 67, 14240, 269, 508, 67, 3645, 269, 508, 67, 2722, 269, 508, 67, 10822, 269, 3119, 269, 2072, 269, 324, 2323, 269, 1216, 269, 443, 12885, 269, 727, 67, 13957, 269, 325, 303, 67, 350, 269, 7129, 1734, 67, 350, 571, 779, 79, 294, 779, 79, 67, 350, 269, 779, 79, 67, 529, 269, 779, 79, 67, 4930, 269, 12797, 269, 919, 269, 5251, 571, 14486, 310, 294, 7291, 67, 350, 269, 3286, 269, 384, 474, 269, 5927, 67, 350, 261, 432, 5967, 262, 269, 884, 20910, 67, 350, 261, 18520, 262, 269, 341, 269, 328, 269, 314, 269, 10763, 269, 14947, 269, 699, 83, 269, 5893, 269, 2359, 20125, 269, 366, 269, 6445, 269, 15407, 269, 7129, 269, 1427, 269, 9846, 556, 84, 269, 25120, 269, 9834, 70, 269, 6357, 269, 366, 17152, 269, 22723, 269, 324, 7944, 269, 13828, 269, 436, 269, 699, 269, 9033, 269, 314, 67, 350, 84, 571, 14486, 310, 67, 2767, 30321, 294, 7291, 67, 350, 269, 3286, 269, 3643, 269, 5927, 67, 350, 261, 432, 5967, 262, 269, 884, 20910, 67, 350, 261, 18520, 262, 269, 341, 269, 328, 269, 314, 269, 10763, 269, 14947, 269, 699, 83, 269, 5893, 269, 2359, 20125, 269, 366, 269, 6445, 269, 15407, 269, 7129, 269, 1427, 269, 9846, 556, 84, 269, 25120, 269, 9834, 70, 269, 6357, 269, 366, 17152, 269, 22723, 269, 324, 7944, 269, 13828, 269, 436, 269, 699, 269, 9033, 269, 314, 67, 350, 84, 571, 12814, 814, 294, 3286, 269, 5927, 67, 350, 261, 432, 5967, 262, 269, 884, 20910, 67, 350, 261, 18520, 262, 269, 7291, 67, 350, 269, 12814, 814, 571, 645, 298, 908, 294, 645, 298, 908, 67, 350, 269, 508, 67, 2854, 269, 12797, 269, 919, 269, 5251, 571, 1603, 30321, 294, 3286, 269, 3643, 269, 5927, 67, 350, 67, 91, 7872, 261, 432, 5967, 262, 269, 884, 20910, 67, 350, 67, 91, 7872, 261, 18520, 262, 269, 5927, 67, 350, 67, 383, 550, 261, 432, 5967, 262, 269, 884, 20910, 67, 350, 67, 383, 550, 261, 18520, 262, 269, 31307, 269, 24528, 269, 268, 606, 571, 5927, 294, 3286, 269, 884, 20910, 67, 350, 261, 18520, 262, 269, 5927, 67, 350, 261, 432, 5967, 262, 269, 284, 2645, 784, 67, 350, 261, 432, 5967, 262, 269, 3739, 67, 350, 269, 6171, 269, 314, 269, 314, 8712, 269, 341, 269, 328, 269, 3739, 67, 8082, 269, 14310, 67, 8082, 269, 15266, 67, 8082, 269, 4945, 67, 8082, 269, 436, 269, 1223, 269, 366, 269, 1645, 269, 14543, 269, 15407, 269, 7129, 269, 1427, 269, 2393, 269, 2873, 269, 366, 17152, 269, 9033, 269, 767, 269, 6445, 269, 25120, 269, 14947, 269, 699, 83, 269, 5893, 269, 2359, 20125, 269, 10677, 269, 366, 354, 269, 7129, 69, 269, 1427, 69, 269, 425, 269, 9986, 269, 4253, 269, 508, 269, 779, 79, 269, 2403, 409, 1359, 269, 9107, 74, 269, 8228, 74, 269, 5927, 67, 350, 67, 2848, 261, 432, 5967, 262, 269, 5927, 67, 350, 67, 80, 9795, 4728, 7950, 261, 432, 5967, 262, 269, 5927, 67, 350, 67, 1349, 303, 261, 432, 5967, 262, 571, 5927, 67, 74, 2645, 784, 294, 284, 2645, 784, 67, 350, 261, 432, 5967, 262, 269, 284, 2645, 784, 67, 529, 269, 2695, 269, 9583, 67, 14578, 261, 432, 5967, 262, 571, 5927, 67, 20222, 294, 3286, 269, 884, 20910, 67, 350, 261, 18520, 262, 269, 5927, 67, 350, 261, 432, 5967, 262, 269, 8816, 269, 3739, 67, 350, 269, 3739, 67, 8082, 269, 6171, 269, 314, 269, 341, 269, 328, 269, 987, 326, 890, 9742, 12814, 5646, 434, 18115, 316, 326, 18520, 316, 4044, 21, 18, 2], 
    #                 dtype=torch.long).reshape(1,-1)
    # output_ids = torch.tensor([1, 9111, 12814, 814, 4571, 12814, 814, 4852, 3286, 273, 4044, 21, 4116, 884, 20910, 67, 350, 273, 315, 24924, 6, 10205, 6953, 12814, 814, 14326, 13319, 890, 2], 
    #                 dtype=torch.long).reshape(1,-1)

    input_ids = torch.load("/home/estengel/scratch/fed_input.pt")
    output_ids = torch.load("/home/estengel/scratch/fed_labels.pt")

    io = {"input_ids": input_ids, "labels": output_ids}
    output = model(**io) 
    print(output)