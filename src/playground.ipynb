{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_parsing_with_constrained_lm.scfg.scfg import SCFG\n",
    "from semantic_parsing_with_constrained_lm.scfg.generate import parse_and_render\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scfg = SCFG.from_file(\"/home/estengel/scratch/scfg_playground/pp.scfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_grammar = scfg.utterance_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_parsing_with_constrained_lm.scfg.parser.token import TerminalToken, NonterminalToken\n",
    "\n",
    "def expand(grammar, nonterminal):\n",
    "    return [x[0] for x in grammar[nonterminal]]\n",
    "\n",
    "def check_all_terminal(sequence):\n",
    "    if len(sequence) == 0:\n",
    "        return False\n",
    "    for s in sequence:\n",
    "        if not isinstance(s, TerminalToken):\n",
    "            return False\n",
    "    return True \n",
    "\n",
    "def generate(grammar, symbol, strings=[]): \n",
    "    \"\"\"Generate all possible strings from a lark CFG\n",
    "    \n",
    "    Args:\n",
    "        lark_grammar (lark.Lark): a lark grammar object\n",
    "    Returns:\n",
    "        list of str: all possible strings\n",
    "    \"\"\"\n",
    "    \n",
    "    def helper(grammar, symbol):\n",
    "        # case 1: symbol is terminal\n",
    "        if isinstance(symbol, TerminalToken):\n",
    "            # add to current string\n",
    "            return symbol.underlying\n",
    "            \n",
    "        # case 2: symbol is nonterminal\n",
    "        elif isinstance(symbol, NonterminalToken):\n",
    "            # expand and repeat \n",
    "            print(f\"expanding {symbol}\")\n",
    "            return [helper(grammar, x) for x in expand(grammar, symbol.underlying)]\n",
    "\n",
    "        elif isinstance(symbol, str):\n",
    "            return [helper(grammar, x) for x in expand(grammar, symbol)]\n",
    "        # case 3: expansion rule has created sequence of options \n",
    "        elif type(symbol) in [tuple,list]:\n",
    "            return [helper(grammar, tok) for tok in symbol]\n",
    "        else:\n",
    "\n",
    "            raise ValueError(f\"Invalid symbol type: {type(symbol)}\")\n",
    "\n",
    "    strings = helper(grammar, symbol)\n",
    "    return strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "strings_to_sample = generate(string_grammar, 'Ambig_PP_sentence_')\n",
    "print(strings_to_sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: take the produce of the nested lists here to sample \n",
    "\n",
    "class EnumSampler:\n",
    "    \"\"\"Sample everything that hasn't already been sampled\"\"\"\n",
    "    def __call__(self, s, done):\n",
    "        for x in s:\n",
    "            if x not in done:\n",
    "                return x\n",
    "        return None\n",
    "\n",
    "done = []\n",
    "\n",
    "def reduce_singleton(s):\n",
    "    if len(s) == 1:\n",
    "        return [s[0]]\n",
    "    lens = [len(x) for x in s]\n",
    "    types = [type(x[0]) for x in s]\n",
    "    if all([x == 1 for x in lens]) and all([x == str for x in types]):\n",
    "        return [x for l in s for x in l ]\n",
    "    return s \n",
    "\n",
    "sampled = []\n",
    "\n",
    "def sample(nested_strs, sampler):\n",
    "    # nested_strs is list of lists, where we can keep going down until we get to a terminal (str)\n",
    "    print(\"Nest, \", nested_strs)\n",
    "    for s in nested_strs:\n",
    "        if len(s) == 0:\n",
    "            return None \n",
    "        # reduce singleton lists \n",
    "        s = reduce_singleton(s)\n",
    "        if isinstance(s[0], str):\n",
    "            samp = sampler(s, done)\n",
    "            if samp is None:\n",
    "                return None\n",
    "            done.append(samp)\n",
    "            sampled.append(samp)\n",
    "        else:\n",
    "            sample(s, sampler) \n",
    "\n",
    "print(sample(strings_to_sample, EnumSampler()))\n",
    "print(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_singleton([['\"the boy\"'], ['\"Galileo\"'], ['\"the girl\"'], ['\"the man\"']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# let's test grammar on parsing\n",
    "\n",
    "input_str = \" the boy observed Galileo with binoculars \"\n",
    "interpretation = parse_and_render(scfg, input_str, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "input_str_pp = \"The man saw the boy with the telescope\"\n",
    "# output_str_pp1 = \"(let (e1 SAW e2 HAVE x BOY) (AND (AGENT e1 MAN) ( PATIENT e1 x ) ( AGENT e2 x ) ( PATIENT e2 TELESCOPE )))\"\n",
    "output_str_pp1 = \"(let (e1 SAW e2 HAVE x BOY) (AND (AGENT e1 MAN) ( PATIENT e1 x ) ))\"\n",
    "output_str_pp2 = \"(let (e1 SAW) (AND ((AGENT e1 MAN) ( PATIENT e1 BOY ) ( INSTRUMENT e1 TELESCOPE ) ) ))\"\n",
    "# output_str_pp1 = \"(Yield (FindEventWrapperWithDefaults (EventOnDateWithTimeRange (EventOnDate (NextDOW (Wednesday)) (^(Event) EmptyStructConstraint)) (Afternoon))))\"\n",
    "# output_str_pp2 = \"(Yield (FindEventWrapperWithDefaults (EventOnDateWithTimeRange (EventOnDate (NextDOW (Wednesday)) (^(Event) EmptyStructConstraint)) (Afternoon))))\"\n",
    "\n",
    "template = {\"dialogue_id\": \"None\",\n",
    "            \"turns\": [{\"fully_typed_lispress\": None,\n",
    "                       \"lispress\": None,\n",
    "                       \"program_execution_oracle\":{\"has_exception\":False,\"refer_are_correct\":True},\n",
    "                       \"skip\":False,\n",
    "                       \"turn_index\":0,\n",
    "                       \"user_utterance\": {\"original_text\":\"\", \n",
    "                       \"tokens\":[]},\n",
    "                       \"agent_utterance\":{\"described_entities\":[],\"original_text\":\"hello\",\"tokens\":[\"hello\"]}\n",
    "                    }, {\"fully_typed_lispress\": None,\n",
    "                        \"lispress\": None,\n",
    "                       \"program_execution_oracle\":{\"has_exception\":False,\"refer_are_correct\":True},\n",
    "                       \"skip\":False,\n",
    "                       \"turn_index\":0,\n",
    "                       \"user_utterance\": {\"original_text\":\"\", \n",
    "                       \"tokens\":[]},\n",
    "                       \"agent_utterance\":{\"described_entities\":[],\"original_text\":\"hello\",\"tokens\":[\"hello\"]}\n",
    "                    }]\n",
    "}\n",
    "\n",
    "entry_0 = {k:v for k, v in template.items()}\n",
    "entry_0['dialogue_id'] = \"00\"\n",
    "entry_0['turns'][0]['user_utterance']['original_text'] = input_str_pp\n",
    "entry_0['turns'][0]['user_utterance']['tokens'] = input_str_pp.split(\" \")\n",
    "entry_0['turns'][0]['fully_typed_lispress'] = output_str_pp1\n",
    "entry_0['turns'][0]['lispress'] = output_str_pp1\n",
    "\n",
    "entry_0 = {k:v for k, v in template.items()}\n",
    "entry_0['dialogue_id'] = \"00\"\n",
    "entry_0['turns'][1]['user_utterance']['original_text'] = input_str_pp\n",
    "entry_0['turns'][1]['user_utterance']['tokens'] = input_str_pp.split(\" \")\n",
    "entry_0['turns'][1]['fully_typed_lispress'] = output_str_pp2\n",
    "entry_0['turns'][1]['lispress'] = output_str_pp2\n",
    "\n",
    "\n",
    "\n",
    "with open(\"/home/estengel/semantic_parsing_with_constrained_lm/src/semantic_parsing_with_constrained_lm/domains/ambig/data/example.jsonl\", \"w\") as f1:\n",
    "    for line in [entry_0]:\n",
    "        f1.write(json.dumps(line) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting grammar ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not parse: ['AND', ['AGENT', 'e1', 'MAN'], ['PATIENT', 'e1', 'x']]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7493/751893901.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0;34m\"/home/estengel/semantic_parsing_with_constrained_lm/src/semantic_parsing_with_constrained_lm/domains/ambig/data/example.jsonl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0;34m\"/home/estengel/semantic_parsing_with_constrained_lm/src/semantic_parsing_with_constrained_lm/domains/ambig/grammar\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0mwhitelisted_dialogue_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             )\n",
      "\u001b[0;32m~/semantic_parsing_with_constrained_lm/src/semantic_parsing_with_constrained_lm/domains/lispress_v2/create_benchclamp_data.py\u001b[0m in \u001b[0;36mextract_and_write_grammar\u001b[0;34m(train_dataflow_dialogues_jsonl, grammar_output_dir, whitelisted_dialogue_ids)\u001b[0m\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m     grammar_rules = extract_grammar(\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mdataflow_dialogues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhitelisted_dialogue_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhitelisted_dialogue_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# Write Grammar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/semantic_parsing_with_constrained_lm/src/semantic_parsing_with_constrained_lm/domains/lispress_v2/grammar.py\u001b[0m in \u001b[0;36mextract_grammar\u001b[0;34m(dataflow_dialogues, whitelisted_dialogue_ids)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mturn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataflow_dialogue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mlispress_expr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_fully_typed_lispress_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mturn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfully_typed_lispress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mgrammar_rules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_grammar_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlispress_expr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mroot_type_nt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_nt_from_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlispress_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/semantic_parsing_with_constrained_lm/src/semantic_parsing_with_constrained_lm/domains/lispress_v2/lispress_exp.py\u001b[0m in \u001b[0;36mparse_fully_typed_lispress_v2\u001b[0;34m(lispress)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_fully_typed_lispress_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlispress\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mLispressExpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0msexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_lispress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlispress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mlispress_expr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_fully_typed_lispress_v2_sexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlispress_expr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/semantic_parsing_with_constrained_lm/src/semantic_parsing_with_constrained_lm/domains/lispress_v2/lispress_exp.py\u001b[0m in \u001b[0;36mparse_fully_typed_lispress_v2_sexp\u001b[0;34m(sexp)\u001b[0m\n\u001b[1;32m    171\u001b[0m         return LetExpr(\n\u001b[1;32m    172\u001b[0m             \u001b[0mvar_assignments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar_assignments\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mmain_expr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_fully_typed_lispress_v2_sexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_expr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         )\n",
      "\u001b[0;32m~/semantic_parsing_with_constrained_lm/src/semantic_parsing_with_constrained_lm/domains/lispress_v2/lispress_exp.py\u001b[0m in \u001b[0;36mparse_fully_typed_lispress_v2_sexp\u001b[0;34m(sexp)\u001b[0m\n\u001b[1;32m    214\u001b[0m             )\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not parse: {sexp}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: Could not parse: ['AND', ['AGENT', 'e1', 'MAN'], ['PATIENT', 'e1', 'x']]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION']  = \"python\"\n",
    "from semantic_parsing_with_constrained_lm.domains.ambig.create_benchclamp_data import extract_and_write_grammar\n",
    "\n",
    "extract_and_write_grammar(\n",
    "                \"/home/estengel/semantic_parsing_with_constrained_lm/src/semantic_parsing_with_constrained_lm/domains/ambig/data/example.jsonl\",\n",
    "                \"/home/estengel/semantic_parsing_with_constrained_lm/src/semantic_parsing_with_constrained_lm/domains/ambig/grammar\",\n",
    "                whitelisted_dialogue_ids=None,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let',\n",
       " ['e1', 'SAW', 'e2', 'HAVE', 'x', 'BOY'],\n",
       " ['&',\n",
       "  [['AGENT', 'e1', 'MAN'],\n",
       "   ['PATIENT', 'e1', 'x'],\n",
       "   ['AGENT', 'e2', 'x'],\n",
       "   ['PATIENT', 'e2', 'TELESCOPE']]]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataflow.core.lispress import parse_lispress\n",
    "\n",
    "parse_lispress(\"(let (e1 SAW e2 HAVE x BOY) (& ((AGENT e1 MAN) ( PATIENT e1 x ) ( AGENT e2 x ) ( PATIENT e2 TELESCOPE ))))\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(let (e1 SAW) (& ((AGENT e1 AMAN) ( PATIENT e1 BOY ) ( INSTRUMENT e1 TELESCOPE ) ) ))\n",
    "\n",
    "(& ( (AGENT e1 MAN) ( PATIENT e1 x ) ( AGENT e2 x ) ( PATIENT e2 TELESCOPE )))\n",
    "\n",
    "(let (e1 SAW e2 HAVE x BOY) (& ((AGENT e1 MAN) ( PATIENT e1 x ) ( AGENT e2 x ) ( PATIENT e2 TELESCOPE ))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('bclamp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5838cd6d3aa9395e77de8eb86a5b18574c2a5a3b61b7b1f2baa99b5eb005498"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
